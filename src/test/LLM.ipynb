{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd727a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "current_dir = %pwd\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '../..'))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a44988-38a7-48d1-a7da-43f39fb29398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import outlines\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "access_token = \"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\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c6c8e2-aa6f-4d94-a419-09744966179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.utilities.utils import get_dataset\n",
    "\n",
    "inputs, targets = get_dataset(one_hot=False)\n",
    "\n",
    "choices = [\"Entertainment\", \"Life\", \"Politics\", \"Sports\", \"Voices\"]\n",
    "\n",
    "\n",
    "def make_prompt(sentence, choices):\n",
    "    prompt = f\"\"\"Classify this news in one of the category: '{sentence}'.\n",
    "    Choose between the following categories: {\", \".join(choices)}.\n",
    "    Answer: \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f988fe-90f1-407f-aa6d-4165d5457934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", model = model_id, model_kwargs ={\"torch_dtype\": torch.float16}, device_map = \"auto\", do_sample= False, token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e3712-98ed-4633-93fb-4a4c68e5a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_replies = []\n",
    "for news in inputs:\n",
    "    prompt = make_prompt(news, choices) \n",
    "    reply = pipe(prompt, max_new_tokens = 20, return_full_text = False)\n",
    "    model_replies.append(reply[0]['generated_text'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e3bad-8844-4a3e-a1b4-a9a899505598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# put everything in a dict and convert to dataframe\n",
    "d = {\"sentence\": news, \"model reply\": model_replies}\n",
    "df = pd.DataFrame(d)\n",
    "print(df['model reply'][0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459399c7-de99-4e80-90b9-b840e41b70e8",
   "metadata": {},
   "source": [
    "### Use  [Outlines](https://outlines-dev.github.io/outlines/) for forcing specific tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c5922-2807-4fc7-a4b8-5d3f589397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe # clean up memory of model\n",
    "\n",
    "# load the model with outlines\n",
    "model = outlines.models.transformers(model_id, model_kwargs ={\"torch_dtype\": torch.float16}, device=\"cuda\")\n",
    "\n",
    "# load a generator with the model and possible choices\n",
    "generator = outlines.generate.choice(model, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496995c-23fd-4ef4-8208-f468a0c3b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlines_replies = []\n",
    "for news in inputs:\n",
    "    prompt = make_prompt(news, choices) \n",
    "    # generate reply with generator (will be one of choices)\n",
    "    reply = generator(prompt)\n",
    "    outlines_replies.append(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c06d0-7366-49fe-bb2a-1e13b6fee1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['outlines replies'] = outlines_replies\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736f54f-4510-454e-ad8b-92f2917abde6",
   "metadata": {},
   "source": [
    "## Multilabel classification w/ LLM prompt and outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d17ccd-0499-464c-b554-9723c4ae44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache\n",
    "import outlines\n",
    "from tqdm import tqdm\n",
    "\n",
    "#del pipe # clean up memory of model\n",
    "#del model #clean up model\n",
    "#del generator #clean up generator\n",
    "\n",
    "\n",
    "# load the model with outlines\n",
    "model = outlines.models.transformers(model_id, model_kwargs ={\"torch_dtype\": torch.float16}, device=\"cuda\")\n",
    "\n",
    "\n",
    "# load a generator with the model and possible choices\n",
    "choices = ['True', 'False']\n",
    "generator = outlines.generate.choice(model, choices)\n",
    "\n",
    "labels = [\"Entertainment\", \"Life\", \"Politics\", \"Sports\", \"Voices\"]\n",
    "\n",
    "def make_prompt(news, choices):\n",
    "    prompt = f\"\"\"Classify this news in one of the category: '{news}'.\n",
    "    Choose between the following categories: {\", \".join(choices)}.\n",
    "    Answer {\" or \".join(choices)}.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "outlines_replies = []\n",
    "for sentence in tqdm(news):\n",
    "    prompts = make_prompt(sentence, labels, choices)\n",
    "    # generate reply with generator (will be one of choices)\n",
    "    predictions = generator(prompts)\n",
    "    #predictions = [True if \"si\" else False for e in predictions]\n",
    "    outlines_replies.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561adbe-a6a9-4064-bc33-9a835d6317ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown(f\"\"\"{\"# <center>LlaMA-3</center>\"}\"\"\"))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame(outlines_replies, columns = labels)\n",
    "df['sentence'] = news\n",
    "display(df[[\"Entertainment\", \"Life\", \"Politics\", \"Sports\", \"Voices\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T11:48:31.101453Z",
     "start_time": "2024-05-24T11:48:31.096100Z"
    }
   },
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "current_dir = %pwd\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '../..'))\n",
    "sys.path.append(parent_dir)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from src.main.models.transformers import Transformer\n",
    "from config import config\n",
    "from src.main.pipeline.functions import clean_text, stop_words_removal\n",
    "from src.main.utilities.utils import get_dataset, split_train_val_test\n",
    "import numpy as np\n",
    "\n",
    "inputs, targets = get_dataset(one_hot=False)\n",
    "\n",
    "checkpoint = \"roberta-base\"\n",
    "\n",
    "training_args = {\n",
    "    'output_dir': config.RESULTS_DIRECTORY.format(checkpoint),\n",
    "    'num_train_epochs': 5,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 500,\n",
    "    'weight_decay': 0.01,\n",
    "    'logging_dir': config.LOGS_PATH.format(checkpoint),\n",
    "    'logging_steps': 500,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'accuracy'\n",
    "}\n",
    "\n",
    "roberta = Transformer(checkpoint=checkpoint, **training_args)\n",
    "roberta.pipeline = [clean_text, stop_words_removal]\n",
    "inputs = roberta.run_pipeline(inputs, save=True)\n",
    "targets = np.vectorize(config.label2id.get)(targets)\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_train_val_test(inputs, targets, validation_size=0.2, test_size=0.1)\n",
    "\n",
    "roberta.prepare_dataset(x_train, x_val, y_train, y_val)\n",
    "roberta.fit(x_train, y_train)\n",
    "\n",
    "test_dataset = roberta.prepare_test_dataset(x_test, y_test)\n",
    "roberta.save_results(test_dataset)\n",
    "roberta.save_model()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
